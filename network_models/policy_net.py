import tensorflow as tf


class Policy_net:
    def __init__(self, name: str, env):
        """
        name: ネットワークの名前
        env: gymの環境
        """

        # 状態集合
        ob_space = env.observation_space
        # 行動集合
        act_space = env.action_space

        with tf.variable_scope(name):
            # 観測したtrajectoriesのプレースホルダー
            self.obs = tf.placeholder(dtype=tf.float32, shape=[None] + list(ob_space.shape), name='obs')

            # 方策用
            with tf.variable_scope('policy_net'):
                # 入力: 状態, 出力: 行動
                layer_1 = tf.layers.dense(inputs=self.obs, units=20, activation=tf.tanh)
                layer_2 = tf.layers.dense(inputs=layer_1, units=20, activation=tf.tanh)
                layer_3 = tf.layers.dense(inputs=layer_2, units=act_space.n, activation=tf.tanh)
                self.act_probs = tf.layers.dense(inputs=layer_3, units=act_space.n, activation=tf.nn.softmax)

            # 収益用
            with tf.variable_scope('value_net'):
                # 入力: 状態, 出力: 収益
                layer_1 = tf.layers.dense(inputs=self.obs, units=20, activation=tf.tanh)
                layer_2 = tf.layers.dense(inputs=layer_1, units=20, activation=tf.tanh)
                self.v_preds = tf.layers.dense(inputs=layer_2, units=1, activation=None)

            # 行動の分布から確率的に行動を選択
            self.act_stochastic = tf.multinomial(tf.log(self.act_probs), num_samples=1)
            self.act_stochastic = tf.reshape(self.act_stochastic, shape=[-1])

            # 行動の分布から決定的に行動を選択つまり最大値のインデックス
            self.act_deterministic = tf.argmax(self.act_probs, axis=1)

            self.scope = tf.get_variable_scope().name

    def act(self, obs, stochastic=True):
        '''
        obs: 観測
        stochastic: 確率的な方策を用いるかどうか
        return: 行動と収益の期待値
        '''
        if stochastic:
            return tf.get_default_session().run(
                    [self.act_stochastic, self.v_preds],
                    feed_dict={self.obs: obs})
        else:
            return tf.get_default_session().run(
                    [self.act_deterministic, self.v_preds],
                    feed_dict={self.obs: obs})

    def get_action_prob(self, obs):
        '''
        obs: 観測
        return: 行動の分布
        '''
        return tf.get_default_session().run(
                self.act_probs,
                feed_dict={self.obs: obs})

    def get_variables(self):
        '''ネットワークの全パラメータ取得'''
        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.scope)

    def get_trainable_variables(self):
        '''学習対象のネットワークのパラメータ取得'''
        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)
